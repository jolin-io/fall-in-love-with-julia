{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/schlichtanders/fall-in-love-with-julia/main?filepath=05%20machinelearning%20with%20MLJ%20-%2001%20introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning in Julia with MLJ\n",
    "\n",
    "Welcome to this little Jupyter Notebook for getting to know MLJ, the goto ML platform within Julia.\n",
    "\n",
    "To start with, take a look at [MLJ's github page](https://github.com/alan-turing-institute/MLJ.jl):\n",
    "* super well organized: own [Github Organization \"JuliaAI\"](https://github.com/JuliaAI)\n",
    "* well maintained and supported: see the maintainers and support below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -----------------------------\n",
    ">\n",
    "> <div align=\"center\">\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/MLJLogo2.svg\" alt=\"MLJ\" width=\"200\">\n",
    "> </div>\n",
    "> \n",
    "> <h2 align=\"center\">A Machine Learning Framework for Julia\n",
    "> </h2>\n",
    "> \n",
    "> \n",
    "> MLJ (Machine Learning in Julia) is a toolbox written in Julia\n",
    "> providing a common interface and meta-algorithms for selecting,\n",
    "> tuning, evaluating, composing and comparing over [160 machine learning\n",
    "> models](https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/)\n",
    "> written in Julia and other languages.\n",
    "> \n",
    "> **New to MLJ?** Start [here](https://alan-turing-institute.github.io/MLJ.jl/dev/).\n",
    "> \n",
    "> **Integrating an existing machine learning model into the MLJ\n",
    "> framework?** Start [here](https://alan-turing-institute.github.io/MLJ.jl/dev/quick_start_guide_to_adding_models/).\n",
    "> \n",
    "> MLJ was initially created as a Tools, Practices and Systems project at\n",
    "> the [Alan Turing Institute](https://www.turing.ac.uk/)\n",
    "> in 2019. Current funding is provided by a [New Zealand Strategic\n",
    "> Science Investment\n",
    "> Fund](https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/)\n",
    "> awarded to the University of Auckland.\n",
    "> \n",
    "> MLJ been developed with the support of the following organizations:\n",
    "> \n",
    "> <div align=\"center\">\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/Turing_logo.png\" width = 100/>\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/UoA_logo.png\" width = 100/>\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/IQVIA_logo.png\" width = 100/>\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/warwick.png\" width = 100/>\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/julia.png\" width = 100/>\n",
    "> </div>\n",
    "> \n",
    "> \n",
    "> ### The MLJ Universe\n",
    "> \n",
    "> The functionality of MLJ is distributed over a number of repositories\n",
    "> illustrated in the dependency chart below. These repositories live at\n",
    "> the [JuliaAI](https://github.com/JuliaAI) umbrella organization.\n",
    "> \n",
    "> <div align=\"center\">\n",
    ">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/dev/material/MLJ_stack.svg\" alt=\"Dependency Chart\">\n",
    "> </div>\n",
    "> \n",
    "> *Dependency chart for MLJ repositories. Repositories with dashed\n",
    "> connections do not currently exist but are planned/proposed.*\n",
    "> \n",
    "> <br>\n",
    "> <p align=\"center\">\n",
    "> <a href=\"CONTRIBUTING.md\">Contributing</a> &nbsp;•&nbsp; \n",
    "> <a href=\"ORGANIZATION.md\">Code Organization</a> &nbsp;•&nbsp;\n",
    "> <a href=\"ROADMAP.md\">Road Map</a> \n",
    "> </br>\n",
    "> \n",
    "> #### Contributors\n",
    "> \n",
    "> *Core design*: A. Blaom, F. Kiraly, S. Vollmer\n",
    "> \n",
    "> *Lead contributor*: A. Blaom\n",
    "> \n",
    "> *Active maintainers*: A. Blaom, S. Okon, T. Lienart, D. Aluthge\n",
    "> \n",
    ">\n",
    "> ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Many examples and text snippets are taken directly from documentation and examples provided by MLJ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's jump into it: Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier = @iload DecisionTreeClassifier  # interactive model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree  # declaritive model loading\n",
    "tree = DecisionTreeClassifier()  # instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ is essentially a big wrapper providing unified access to other packages containing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RDatasets\n",
    "iris = RDatasets.dataset(\"datasets\", \"iris\"); # a DataFrame\n",
    "y, X = unpack(iris, ==(:Species), colname -> true); # y = a vector, and X = a DataFrame \n",
    "first(X, 3) |> pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Fit & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(tree, X, y)  # adding a mutable cache to the model+data for performant training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = partition(eachindex(y), 0.7, shuffle=false); # 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]);\n",
    "yhat[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "isa(yhat[1], Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distributions.mode.(yhat[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(yhat, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in measures()\n",
    "    if \"log_loss\" in m.instances\n",
    "        display(m)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate = auto fit/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(tree, X, y)\n",
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7, shuffle=false),\n",
    "    measures=[log_loss, brier_score], verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.max_depth = 3\n",
    "evaluate!(mach, resampling=CV(shuffle=true), measure=[accuracy, balanced_accuracy], operation=predict_mode, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: fit!, transform, inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 2, 3, 4]\n",
    "mach2 = machine(UnivariateStandardizer(), v)\n",
    "fit!(mach2)\n",
    "w = transform(mach2, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform(mach2, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLJ features\n",
    "\n",
    "\n",
    "MLJ (Machine Learning in Julia) is a toolbox written in Julia\n",
    "providing a common interface and meta-algorithms for selecting,\n",
    "tuning, evaluating, composing and comparing machine learning models\n",
    "written in Julia and other languages. In particular MLJ wraps a large\n",
    "number of [scikit-learn](https://scikit-learn.org/stable/) models. \n",
    "\n",
    "\n",
    "* Data agnostic, train models on any data supported by the\n",
    "  [Tables.jl](https://github.com/JuliaData/Tables.jl) interface,\n",
    "\n",
    "* Extensive support for model composition (*pipelines* and *learning\n",
    "  networks*),\n",
    "\n",
    "* Convenient syntax to tune and evaluate (composite) models.\n",
    "\n",
    "* Consistent interface to handle probabilistic predictions.\n",
    "\n",
    "* Extensible [tuning\n",
    "  interface](https://github.com/alan-turing-institute/MLJTuning.jl),\n",
    "  to support growing number of optimization strategies, and designed\n",
    "  to play well with model composition.\n",
    "\n",
    "\n",
    "More information is available from the [MLJ design paper](https://github.com/alan-turing-institute/MLJ.jl/blob/master/paper/paper.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registry\n",
    "\n",
    "MLJ has a model registry, allowing the user to search models and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "# A more advanced example\n",
    "\n",
    "Disclaimer: This is taken almost completely from an existing MLJ example\n",
    "\n",
    "As in other frameworks, MLJ also supports a variety of unsupervised models for pre-processing data, reducing dimensionality, etc. It also provides a [wrapper](https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/) for tuning model hyper-parameters in various ways. Data transformations, and supervised models are then typically combined into linear [pipelines](https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/#Linear-pipelines-1). However, a more advanced feature of MLJ not common in other frameworks allows you to combine models in more complicated ways. We give a simple demonstration of that next.\n",
    "\n",
    "We start by loading the model code we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeRegressor = @load RidgeRegressor pkg=MultivariateStats\n",
    "RandomForestRegressor = @load RandomForestRegressor pkg=DecisionTree;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define \"learning network\" - a kind of blueprint for the new composite model type. Later we \"export\" the network as a new stand-alone model type. Learning networks can be seen as pipelines on steroids.\n",
    "\n",
    "Let's consider the following simple DAG:\n",
    "![graph](https://alan-turing-institute.github.io/DataScienceTutorials.jl/assets/diagrams/composite1.svg)\n",
    "\n",
    "Our learing network will:\n",
    "\n",
    "- standarizes the input data\n",
    "\n",
    "- learn and apply a Box-Cox transformation to the target variable\n",
    "\n",
    "- blend the predictions of two supervised learning models - a ridge regressor and a random forest regressor; we'll blend using a simple average (for a more sophisticated stacking example, see [here](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/stacking/))\n",
    "\n",
    "- apply the *inverse* Box-Cox transformation to this blended prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic idea is to proceed as if one were composing the various steps \"by hand\", but to wrap the training data in \"source nodes\" first.** In place of production data, one typically uses some dummy data, to test the network as it is built. When the learning network is \"exported\" as a new stand-alone model type, it will no longer be bound to any data. You bind the exported model to production data when your're ready to use your new model type (just like you would with any other MLJ model).\n",
    "\n",
    "There is no need to `fit!` the machines you create, as this will happen automatically when you *call* the final node in the network (assuming you provide the dummy data).\n",
    "\n",
    "*Input layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some synthetic data:\n",
    "X, y = make_regression(100)\n",
    "y = abs.(y)\n",
    "\n",
    "test, train = partition(eachindex(y), 0.8);\n",
    "\n",
    "# wrap as source nodes:\n",
    "Xs = source(X)\n",
    "ys = source(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First layer and target transformation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model = Standardizer()\n",
    "stand = machine(std_model, Xs)\n",
    "W = MLJ.transform(stand, Xs)\n",
    "\n",
    "box_model = UnivariateBoxCoxTransformer()\n",
    "box = machine(box_model, ys)\n",
    "z = MLJ.transform(box, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = RidgeRegressor(lambda=0.1)\n",
    "ridge = machine(ridge_model, W, z)\n",
    "\n",
    "forest_model = RandomForestRegressor(n_trees=50)\n",
    "forest = machine(forest_model, W, z)\n",
    "\n",
    "ẑ = 0.5*predict(ridge, W) + 0.5*predict(forest, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = inverse_transform(box, ẑ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fitting has been done thus far, we have just defined a sequence of operations. We can test the netork by fitting the final predction node and then calling it to retrieve the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(ŷ);\n",
    "ŷ()[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"export\" the network a new stand-alone model type, we can use a macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@from_network machine(Deterministic(), Xs, ys, predict=ŷ) begin\n",
    "    mutable struct CompositeModel\n",
    "        rgs1 = ridge_model\n",
    "        rgs2 = forest_model\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an instance of our new type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = CompositeModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we made our model mutable, we could change the regressors for different ones.\n",
    "\n",
    "For now we'll evaluate this model on the famous Boston data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = @load_boston\n",
    "evaluate(composite, X, y, resampling=CV(nfolds=6, shuffle=true), measures=[rms, mae])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out more [Data Science Tutorials in Julia](https://alan-turing-institute.github.io/DataScienceTutorials.jl/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out one tutorial of your choice right in here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for being here\n",
    "\n",
    "further information about MLJ in general:\n",
    "* MLJ repository: https://github.com/alan-turing-institute/MLJ.jl\n",
    "* MLJ docs: https://alan-turing-institute.github.io/MLJ.jl/dev/\n",
    "* MLJ tutorials: https://alan-turing-institute.github.io/DataScienceTutorials.jl/\n",
    "\n",
    "further information about MLJ's model composition feature\n",
    "* MLJ docs: https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/\n",
    "* MLJ paper: https://arxiv.org/abs/2012.15505\n",
    "* MLJ tutorial: https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/learning-networks/\n",
    "\n",
    "In case you have more questions or suggestions, always feel welcome to reach out to me at Meetup and Julia User Group Munich, or directly at Stephan.Sahm@gmx.de"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "name": "julia",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}